# Basic info
replicaCount: 1

image:
  repository: streamreactor/elastic6
  tag: 1.2.7
  pullPolicy: IfNotPresent

# Resource management
resources:
  limits:
    memory: 512Mi
  requests:
    memory: 256Mi

# Monitoring
monitoring:
  pipeline: "test" # TODO to be configured
  enabled: true
  port: 9102
  path: "/metrics"

podManagementPolicy: OrderedReady

# kafka ssl
# The key and truststores file data are the base64 encoded contents of the files. YOU MUST PROVIDE THE DATA BASE64 encoded
# and added to the kafka secret and mounted into /mnt/connector-secrets
kafka:
  # replicationFactor for connect topics
  replicationFactor: 2
  securityProtocol:
  ssl:
    enabled: false
    trustStoreFileData:
    trustStorePassword:
    keyStoreFileData:
    keyStorePassword:
  sasl:
    enabled: false
    # keyTabData is the contents kerberos keytab file is using kerberos
    keyTabData: |-

    # jaasFileData is the contents of the kafka jaas file
    jaasFileData: |-

    #GSSAPI, SCRAM or PLAIN
    mechanism: GSSAPI
    # kerberos krb5 contents
    krb5Conf: |-

  bootstrapServers:
    - name: fitness-data-pipeline-cp-kafka-headless
      port: 9092
      sslPort: 9093
      saslSslPort: 9094
      saslPlainTextPort: 9095

schemaRegistries:
  enabled: true
  hosts:
    - host: fitness-data-pipeline-cp-schema-registry
      protocol: http
      port: 8081
      jmxPort: 9102


# secretsProvider is either env (k8), vault (hashicorp) or azure (keyvault)
secretsProvider: env

#javaHeap option
javaHeap: "256M"

# clusterName The connect cluster name. This is the consumer group id for the backing topics
elasticClusterName: elasticsearch

# restPort The rest port of Connect
restPort: 8083

# logLevel The log4j level
logLevel: INFO

# keyConverter The key converter to/from Connects struct
keyConverter: org.apache.kafka.connect.storage.StringConverter

# valueConverter The key converter to/from Connects struct
valueConverter: org.apache.kafka.connect.storage.StringConverter

# connectorClass
connectorClass: com.datamountaineer.streamreactor.connect.elastic6.ElasticSinkConnector

# applicationId name of the connector
applicationId: com.datamountaineer.streamreactor.connect.elastic6.ElasticSinkConnector

# topics to sink
topics: anon

# kcqb KCQL expression describing field selection and routes. type: STRING importance: HIGH
kcql: "INSERT INTO activities SELECT * FROM anon"

# A dictionary allowing to specify SMT's, e.g.
# transforms:
#   transforms: "weeklysplit"
#   transforms_weeklysplit_type: "org.apache.kafka.connect.transforms.TimestampRouter"
#   transforms_weeklysplit_timestamp_format: "yyyy-ww"
#   transforms_weeklysplit_topic_format: "${topic}-${timestamp}"
transforms:

# batchSize How many records to process at one time. As records are pulled from Kafka it can be 100k+ which will not be feasible to throw at Elastic search at once type: INT importance: MEDIUM
batchSize: 4000

# useHttp TCP or HTTP. Elastic4s client type to use, http or tcp, default is tcp. type: STRING importance: MEDIUM
useHttp: tcp

# clusterName Name of the elastic search cluster, used in local mode for setting the connection type: STRING importance: HIGH
clusterName: elasticsearch

# writeTimeout The time to wait in millis. Default is 5 minutes. type: INT importance: MEDIUM
writeTimeout: 300000

# xpackPlugins Provide the full class name for all the plugins you want to enable. type: STRING importance: MEDIUM
# xpackPlugins:

# urlPrefix URL connection string prefix type: STRING importance: LOW
urlPrefix: elasticsearch

# xpackSettings XpackSettings key in the secret. Enable xpack security add on by providing this setting type: PASSWORD importance: MEDIUM
xpackSettings: |-
 "xpack.security.enabled=false"

# url Url including port for Elastic Search cluster node. type: STRING importance: HIGH
url: elasticsearch-discovery:9300

# maxRetries The maximum number of times to try the write again. type: INT importance: MEDIUM
maxRetries: 20

# retryInterval The time in milliseconds between retries. type: INT importance: MEDIUM
retryInterval: 60000

# errorPolicy
# Specifies the action to be taken if an error occurs while inserting the data.
#  There are three available options:
#     NOOP - the error is swallowed
#     THROW - the error is allowed to propagate.
#     RETRY - The exception causes the Connect framework to retry the message. The number of retries is set by connect.cassandra.max.retries.
# All errors will be logged automatically, even if the code swallows them.
#      type: STRING importance: HIGH
errorPolicy: THROW

# enabled Enables the output for how many records have been processed type: BOOLEAN importance: MEDIUM
progressEnabled: true

